---
title: "Diabetes Model | Machine Learning Project"
author: "Elton Costa"
date: "3/2/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
#Installing the required R packages
if(!require(tidyverse))    install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret))        install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggthemes))     install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(ggcorrplot))   install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(magrittr))     install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(rpart))        install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot))   install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(neighbr))      install.packages("neighbr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(ggthemes)
library(ggcorrplot)
library(randomForest)
library(magrittr)
library(rpart)
library(rpart.plot)
library(knitr)
library(neighbr)
```

# 1. Introduction

Diabetes is a group of metabolic disorders characterized by a high blood sugar level over a prolonged period of time. Symptoms often include frequent urination, increased thirst and increased appetite. If left untreated, diabetes can cause many health complications.

As of 2019, an estimated 463 million people had diabetes worldwide (8.8% of the adult population). Rates are similar in women and men. Trends suggest that rates will continue to rise. Diabetes at least doubles a person's risk of early death. In 2019, diabetes resulted in approximately 4.2 million deaths. It is the 7th leading cause of death globally. The global economic cost of diabetes-related health expenditure in 2017 was estimated at US\$727 billion. In the United States, diabetes cost nearly US\$327 billion in 2017. Average medical expenditures among people with diabetes are about 2.3 times higher.[[1](https://www.diabetesatlas.org/upload/resources/material/20200302_133351_IDFATLAS9e-final-web.pdf)]

The internet provides a range symptoms for people to watch out for, however using Google as a self diagnosis tool can be unreliable and, quite frankly, scary. With hospitals being extremely busy during the COVID-19 pandemic, it would impact the life quality if people could find out if they are at risk of being diabetic without having to visit a doctor.

This project aims to develop a machine learning model that predicts whether a patient is at risk of being diabetic. The data being worked with is the [Early stage diabetes risk prediction dataset](https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.). It was created using questionnaires from the patients of Sylhet Diabetic Hospital (Bangladesh) and has been approved by a doctor [[2](https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.)].

Additionally, this document is structured as follows:

1.  Introduction
2.  Method and Data pre-processing
3.  Exploratory Analysis
4.  Model Evaluation
5.  Final Validation
6.  Conclusion



# 2. Method and Data pre-processing

For this report, the Early stage diabetes will be used and can be downloaded from the Machine Learning Repository.

```{r}
#download database from UCI Machine Learning Repository
dl <- tempfile()
download.file(
 "https://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv",
 dl)

#Assign csv database into the data
data <- read_csv(dl, col_types = "dffffffffffffffff")
colnames(data) <- make.names(colnames(data))
```

It is noticeable that the database is ordered by genre. This pre-ordering of the data will impact the outcome of the models. So first it is necessary to reorder the "database.

```{r}
# this loop changes the order of the factors for which the first entry is "No"
no_ind <- which(data[1,]=="No")
for (i in no_ind) {
  data[,i] <- factor(data[[i]], levels = c("Yes","No"))
}
```

The Early stage diabetes risk prediction dataset is split into a training and a validation set (`diabetes` and `validation` respectively). Only the `diabetes` data set is used for model construction. The `validation` data set is used only for assessing the performance of the *final* model. `diabetes` is split into `train` and `test`. Various models are constructed using `train` and their performances are assessed using `test`. The best performing model is then retrained using `diabetes` and assessed using `validation`. This way, `validation` has no effect on which model is selected to be the final model.

`Validation` is 15% of the entire data set and `test` is 15% of `diabetes`. The reason 15% is used for testing and validating in this report is because the data set is quite small. Using 15% instead of 10% for example gives more data to assess the performance of the models.

```{r}
# create a validation set - this is used to assess the final model
# diabetes data set is used for model training and selection
set.seed(4)
validation_index <- createDataPartition(data$class, times=1, p=0.15, list=FALSE)
validation <- data[validation_index,]
diabetes <- data[-validation_index,]
```

Before we start developing models, we will need to create train and test sets from diabetes. The train is used to construct various models and test is used to assess their performances. The best performing model will then be retrained using the diabetes data set and assessed using the validation data set.

```{r}
set.seed(16)
test_index <- createDataPartition(diabetes$class, times=1, p=0.15, list=FALSE)
test <- diabetes[test_index,]
train <- diabetes[-test_index,]
```


# 3. Exploratory Analysis

Before start building the model, we need to understand the structure of the data, the distribution of ratings and the relationship of the predictors. This information will help build a better model.

The structure of the dataset `diabetes` is shown below. "Class" is the predictor variable - "positive" indicates the patient has diabetes. The features are made up of age, gender (biological sex) and a selection of conditions including obesity, alopecia and muscle stiffness. The data contains observations from 272 diabetic and 170 non-diabetic patients. Thus, the prevalence of the condition in the data set does not reflect true prevalence, since less than 10% people are estimated to be diabetic.

```{r, echo = FALSE}
str(diabetes)
```

Figure 1 below shows a correlation plot of the features in the data set. [Polydipsia](https://en.wikipedia.org/wiki/Polydipsia) and [polyuria](https://en.wikipedia.org/wiki/Polyuria) have the greatest correlation, which doesn't come as a surprise. The non-significant correlations are left blank. Although some of the features have high correlations, no dimensional reduction takes place in this report. This is primarily because the data set is quite small so it isn't necessary. Another reason is that [Approach 3: Decision Tree] is much more appealing with interpretable features.

```{r, echo = FALSE, warning=FALSE,fig.align='center'}
bin_diabetes <- sapply({diabetes[,c(-1,-2,-17)] == "Yes"} %>% as_tibble, as.numeric) %>% 
                as_tibble %>% mutate(Gender = as.numeric(diabetes$Gender=="Male"))
correlation_matrix <- round(cor(bin_diabetes),1)

ggcorrplot(correlation_matrix, 
           method = "circle",
           insig = "blank",
           type = "lower",
           title = "Correlation of Diabetes variables", 
           legend.title = "Correlation",
           ggtheme = theme_gdocs())
```

            Figure 1 - Correlation plot of features in the diabetes data set.

This data set suggests that diabetes is more prevalent in females than is it in males, as illustrated in Figure 2. This is perhaps not in line with expectations, as research suggests that men are more likely to develop diabetes than women [[3](https://www.nhs.uk/news/diabetes/men-develop-diabetes-more-easily/)]. This is a reminder that data does not always accurately represent the population it was sampled from. The data set being worked with in this report only accounts for patients from one hospital in Bangladesh, so it would not be wise to make conclusions about diabetes on a world-wide scale.

```{r, echo = FALSE, warning=FALSE,fig.align='center'}
diabetes %>%
  ggplot(aes(class, fill = Gender))+
  geom_bar(width = 0.6, position = position_dodge(width = 0.7))+
  ylab("Number of Patients")+
  ggtitle("Number of Diabetic and Non-Diabet Patients by Gender")
```

            Figure 2 - Distribution of class by gender.

The distribution for age for positive and negative classes looks reasonably similar. Figure 3 below indicates that the spread for positive classes may be larger, however for the most part there doesn't appear to be a significant difference.

```{r, echo = FALSE, warning=FALSE, fig.align='center'}
diabetes %>%
  ggplot(aes(Age, class, fill = class))+
  geom_violin(alpha = 0.8)+
  ggtitle("Prevalence of Diabetes by Age")
```

          Figure 3: Distribution of age by class.

Further exploration can be carried out to discover properties of different features. Figure 4 shows the prevalence of diabetes by polydipsia and polyuria. It appears that if a patient has both polydipsia and polyuria then they are very likely to be diabetic. Otherwise, no confident conclusions could be drawn.

```{r, echo = FALSE, warning=FALSE,fig.align='center'}
diabetes %>%
  ggplot(aes(Polyuria, Polydipsia, color = class))+
  geom_jitter(height = 0.2, width = 0.2)+
  ggtitle("Prevalence of Diabetes by Polyuria and Polydipsia ")
```

          Figure 4 - Prevalence of diabetes by polydipsia and polyuria.

Figure 5 suggests that weakness isn't a condition which is useful in predicting whether a patient is diabetic or not. However, sudden weight loss could be an indication that a patient is diabetic.

```{r, echo = FALSE, warning=FALSE,fig.align='center'}
diabetes %>%
  ggplot(aes(sudden.weight.loss, weakness, colour = class)) +
  geom_jitter(height = 0.2, width = 0.2) +
  xlab("Sudden Weight Loss") +
  ylab("Weakness") +
  ggtitle("Prevalence of Diabetes by Sudden Weight Loss and Weakness")
```

          Figure 5 - Prevalence of diabetes by weakness and sudden wight loss.

Figure 6 is particularly unusual. It suggests that diabetes might not be dependent on obesity. There are numerous studies to suggest that obesity is a significant cause of diabetes, so it is likely that the data set doesn't represent the world population very well.

```{r, echo = FALSE, warning=FALSE,fig.align='center'}
diabetes %>%
  ggplot(aes(class, fill = Obesity))+
  geom_bar(width = 0.6, position = position_dodge(width = 0.7))+
  ylab("Number of Patients")+
  ggtitle("Diabetes by Obesity")
```

          Figure 6 - Distribution of class by obesity.

# 4. Model Evaluation


We will construct a total of 5 models. [Model 1: Logistic Regression] constricts a logistic regression model. [Model 2: k-Nearest Neighbors] Constructs a k-nearest neighbours model. [Model 3: Decision Tree] constructs a decision tree. This method is in line with how people may expect doctors to make decisions in reality. [Model 4: Random Forest] is an extension on [Model 3: Decision Tree]. [Model 5: Ensemble] constructs an ensemble of the three best performing models. [Final Model (Results)] retrains the best performing model on a slightly larger data set and assesses its performance using a validation set which is not used for model construction or selection at any point in this report.


## 4.1: Logistic Regression

In this part of the model development, a logistic regression model will be built. The reason logistic regression is used instead of linear regression is that class is a binary variable. Therefore, it is appropriate for a model to predict the probability that the class of a patient is positive, for example.

The general form of a logistic regression model is

```{=tex}
\begin{equation}
  \log \left(\frac{\hat{\pi}_i}{1-\hat{\pi}_i} \right)=\mathbf{x}_i^T\beta
\end{equation}
```
where $\hat{\pi}_i$ is the estimated probability that observation $i$ is positive, $\mathbf{x}_i$ is the $i^{th}$ vector in the design matrix and $\beta$ is the vector of coefficients. In this case, the first element of $\mathbf{x}_i$ is 1 to activate the intercept in $\beta$, the second element of $\mathbf{x}_i$ is the age of observation $i$, and the rest of the elements are 1-0 dummy variables. For instance, the fourth element of $\mathbf{x}_i$ is 1 if observation $i$ does **not** have polyuria, and is 0 otherwise. This is clear when looking at the summary of the final model towards the end of this section.

Classification models have various measures of performance. One is accuracy, which is the proportion of correctly classified patients. Sensitivity is the proportion of diabetic patients who are correctly classified. High sensitivity implies that a model is likely to correctly classify a diabetic patient. However, this can come at a cost of having low Specificity. Specificity is the proportion of non-diabetic patients that are correctly classified.

One choice that has to be made when constructing a logistic regression model is what cutoff to use. The cutoff $p$ is such that $\hat{\pi}_i>p\Rightarrow$ observation $i$ is classed as positive. A typical choice is 0.5 which will be used in this context.

The summary of the model, trained on `train`, indicates that around half of the features are statistically significant. Non-significant features include obesity, visual blurring and sudden weight loss. Statistically significant features include gender, polyuria and polydipsia.

Below the summary is the confusion matrix (tested on `test`).

```{r, echo = FALSE}
# now use the entire train data set and evaluate the model against the test set
# this section constructs a logistic regression model
model_glm <- glm(as.numeric(class=="Positive")~., family = "binomial", data = train)
preds_glm <- predict(model_glm, test, type = "response")                                                  
preds_glm <- ifelse(preds_glm>0.5, "Positive","Negative") %>% factor(levels = c("Positive","Negative")) 

# confusion matrix using test database
cm_glm <- confusionMatrix(preds_glm, test$class)

# save accuracy and sensitivity in the results tibble that will be our model summary table
acc_glm <- cm_glm$overall["Accuracy"]
sen_glm <- cm_glm$byClass["Sensitivity"]

cm_glm
```

We are going to save and present the results of this and the next models on the table below.We chose sensitivity in addition to accuracy given the importance of sensitivity i.e. performance of the model to correctly classify a diabetic patient.

```{r, echo = FALSE}
results <- tibble(Method = "Model (1) Logistical Regression", Accuracy = acc_glm , Sensitivity = sen_glm )
results %>% knitr::kable()
```
            Table 1: Results after construction of the Logistical Regression model.


## 4.2: k-Nearest Neighbours

The second approach is to construct a k-nearest neighbours model.In principle we want to pick the k that maximizes accuracy. The goal of cross validation is to estimate these quantities for any given algorithm and set of tuning parameters such as k. 

In the model development, a 8-fold cross-validation is used to select the $k$ that will generate the optimal accuracy. The code will use fold from 2 to 8 meaning a total of 7 folds tested. Additionally, we will use the tuneGrid parameter in order to try out 17 values between 2 and 19 neighbours. To do this with caret, we needed to define a column named k, so we use this: data.frame(k = seq(2, 19, 1)). It is known that the value of k as a predictor can be defined as the square root of the number of records in the dataset. In this case, the train dataset has 375 records, and thus k was defined as 19.

That said, the k-nearest neighbors model will be trained 7*18 = 126 times. Given our train dataset is small, this does not require more than a few seconds of processing. However in the case of larger datasets, it would be prudent to run the simulations in a smaller part of the dataset first in order to determine the parameters.

The results are shown in Figure 7 below, highlighting the optimal value of $k=2$.

```{r, echo = FALSE}
# this section constructs a knn model

# the train data set is modified so that the features are logical
# a column ID is also required to run the knn function
train_knn <- {train[-c(1, 2, 17)]=="Yes"} %>% as_tibble # remove age, gender and class. then, convert to logical
train_knn <- cbind(train[2] == "Male", train_knn)       # add logical variable for gender
train_knn <- cbind(train_knn, train[17])                # add the class back in (it doesn't need to be logical, only the features)


# the test set needs to take the same format
# however, the ID and class columns need removed (requirement of knn function)
test_knn <- {test[-c(1, 2, 17)]=="Yes"} %>% as_tibble
test_knn <- cbind(test[2] == "Male", test_knn)

set.seed(4)

# here, 8-fold cross-validation is used to select an optimal k
f = seq(2, 8, 1)

# k-nearest neighbors model trained 7*18 = 126 times
#we try out 8 k from 2 to 8
knn_results <- sapply(f, function(f){
  control <- trainControl(method = "cv", number = f, p = .9)
  train_knn_cv <- train(class ~ ., method = "knn", 
                        data = train_knn,
                        tuneGrid = data.frame(k = seq(2, 19, 1)),
                        trControl = control)
  return(max(train_knn_cv$results$Accuracy))
})

# define the optimal k
n_folds <- which.max(knn_results) + 1

# retrain the knn model
control <- trainControl(method = "cv", number = n_folds, p = .9)
mod_knn <- train(class ~ ., method = "knn", 
                      data = train_knn,
                      tuneGrid = data.frame(k = seq(2, 19, 1)),
                      trControl = control)

#generate chart
ggplot(mod_knn, highlight = TRUE)
```
          Figure 7 - Cross-validation results for kNN model. Optimal k is 1.


Following cross-validation, the `train` data set is used to construct a kNN model using $k=2$. The confusion matrix is shown below. 

```{r, echo = FALSE}

# generate the predictable results using test dataset
preds_knn <- predict(mod_knn, test_knn)

# confusion knn matrix
cm_knn <- confusionMatrix(preds_knn, test$class)
cm_knn
```

Table 2 indicates that the accuracy is the same as the logistic regression model, however the sensitivity is slightly lower. That said, sensitivity is important when predicting diabetes as we want to reduce false negatives i.e. people with diabetes testing negative. With that consideration, logistic is a better model because it presented higher sensitivity.


```{r, echo = FALSE}
# save accuracy and sensitivity knn
acc_knn <- cm_knn$overall["Accuracy"]
sen_knn <- cm_knn$byClass["Sensitivity"]

# store knn results
results <- bind_rows(results, tibble(Method = "Model (2) Knn neighrbours", Accuracy = acc_knn , Sensitivity = sen_knn))
results %>% knitr::kable()
```
       Table 2: Results after construction of the Knn model.


## 4.3: Decision Tree

This section constructs a decision tree. One advantage of decision trees is that they are highly interpretable. Even more so than linear models. The way in which decision trees make classifications is in line with how many people would expect physicians to predict the class of a potentially diabetic patient. 

The rpart package is used to construct the decision tree. However, before the model is constructed, an optimal complexity parameter is chosen (the factor by which the models performance needs to improve by to warrant another split). Bootstrap (25 samples of 25% of the data set) is used to select the optimal complexity parameter. This is the default approach taken by the train function in the caret package. The default minsplit of 20 and minbucket of 7 are used. 

Like the logistic regression model, this decision tree returns probabilities, not classes. Again, some cutoff $p$ is chosen such that $\hat{\pi}_i>p\Rightarrow$ observation $i$ is classed as being diabetic. Figure 8 shows the results from 5-fold cross-validation, highlighting the optimal value of 0.046.

```{r, echo = FALSE}
# this section constructs a decision tree
# this is one the easiest model to interpret, it's easy to visualize how it makes decisions

# the train function in the caret package is used to select an optimal complexity parameter (cp) 
#using 25 bootstrap samples with replacement
set.seed(64)
model_tree <- train(class~.,
                    method = "rpart",
                    tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                    trControl = trainControl(method = "cv", number = 5, p = .9),
                    data = train)

opt_co <- as.numeric(model_tree$bestTune)

# visualize the performance of each cp
ggplot(model_tree, highlight = TRUE)

```
          Figure 8 - Bootstrap (25 samples of 25% of the data) results. Optimal cp is `r round(opt_co, 3)` 



Figure 9 illustrates exactly how the tree makes decisions. The root node makes the first split based on whether the patient has polyuria or not. If they do, they are classed as being diabetic. If not, a further split is made based on the patientâ€™s gender, and so on. The percentage at the bottom of each leaf is the proportion of observations in train that lie in that leaf. The decimal above the percentage is the proportion of observations in that leaf that are non-diabetic.

```{r, echo = FALSE}
# plot the model - this helps to understand how the algorithm works
rpart.plot(model_tree$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)
title("Decision Tree")

```
                              Figure 9 - Decision tree.


The decision tree confusion matrix is shown below. 

```{r, echo = FALSE}
# obtain predictions using opt_p
preds_tree <- predict(model_tree, test) 

# confusion matrix
cm_tree <- confusionMatrix(preds_tree, test$class)
cm_tree
```


Besides its high level of interpretability, Table 3 shows that the decision tree is the best performing model so far with higher accuracy and sensitivity. The next section, [Approach 4: Random Forest], expands on the idea of decision trees.

```{r, echo = FALSE}
# save accuracy and sensitivity
acc_tree <- cm_tree$overall["Accuracy"]
sen_tree <- cm_tree$byClass["Sensitivity"]

results <- bind_rows(results, tibble(Method = "Model (3) Decision tree", Accuracy = acc_tree , Sensitivity = sen_tree))
results %>% knitr::kable()

```
             Table 3: Results after construction of the decision tree model.


## 4.4: Random Forest

This model is an extension of the decision tree - a random forest is a collection of decision trees. The way the random forest makes predictions is by some form of majority vote among all of the trees. Trees are constructed in a similar way as the previous section, however at each node a random subset of features is chosen to make the split. 

This increases the independence between the trees, this parameter is `mtry` in the randomForest package [[4](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf)]. Again, bootstrap (25 samples of 25%) is used to choose an optimal mtry. The results are shown below in Figure 10. The optimal value is 5. The randomForest package takes the default `nodesize` (minimum size of terminal nodes) to be 1 and the default `ntree` (number of decision trees in the forest) to be 500. 

```{r, echo = FALSE}
set.seed(11)

model_rf <- train(class~.,
                  method = "rf",
                  tuneGrid = data.frame(mtry = 3:11),
                  data = train)

#visualize the performance of each mtry
ggplot(model_rf, highlight = TRUE)+
  ggtitle("Accuracy for each number of randomly selected predictors")
```
            Figure 10: Bootstrap results for various values of mtry.

The confusion matrix below indicates that the random forest performs very well in comparison to the previous models.

```{r, echo = FALSE}

#generate predictions
preds_rf <- predict(model_rf, test)

# confusion matrix
cm_rf <- confusionMatrix(preds_rf, test$class)
```

The table below shows the  importance of each variable is also accessible via the importance function. The attribute polyuria is a clear winner, meaning it is likely to be the root note in most of the decision trees in the forest.

```{r, echo = FALSE}
# save rf accuracy and sensitivity
acc_rf <- cm_rf$overall["Accuracy"]
sen_rf <- cm_rf$byClass["Sensitivity"]

# the importance of each variable is also accessible via the importance function
# polyuria is a clear winner, meaning it is likely to be the root note in most of the decision trees in the forest
importance(model_rf$finalModel)

```
        Table 4 - Final random forest model attributes 

The random forest model achieves an accuracy of `r round(acc_rf, 3)` which is better than all prior models. the same is not observed for sensitivity which actually is the same as the decision tree model. Table 5 shows the performances of the first four models combined.

```{r, echo = FALSE}
# store rf results
results <- bind_rows(results, tibble(Method = "Model (4) Random Forest", Accuracy = acc_rf , Sensitivity = sen_rf))
results %>% knitr::kable()
```
              Table 5 - Results after construction of the Random forest model. 


## 4.5: Ensemble

The final model is an ensemble of the three best performing models.The decision tree is not considered as part of the ensemble model given the random forest is supposed to be a better version of a decision tree.

The ensemble takes a majority vote for each observation from the three models (logistic regression, kNN and random forest) and uses that as its prediction. By dropping one of the four models ties are avoided. Ensembling machine learning models is a great strategy to improve accuracy on test sets - it reduces the reliability on the performance of only one algorithm.

The confusion matrix below shows the accuracy of the model which is below than expected.

```{r, echo = FALSE}
# store the predictions from the three models in a data frame

all_preds <- tibble(glm = preds_glm,
                    rf = preds_rf,
                    knn = preds_knn)

# the prediction of the ensemble are defined by the majority
preds_en <- apply(all_preds, 1, function(x) names(which.max(table(x)))) %>%
  factor(levels = c("Positive", "Negative"))

# confusion matrix (it actually performs worse than the random forest)
cm_ens <- confusionMatrix(preds_en, test$class)
```

The result table below surprisly shows that the ensemble performs worse than the random forest. More on why this may be the case is discussed in the Conclusion.

```{r, echo = FALSE}
# save accuracy and sensitivity
acc_ens <- cm_ens$overall["Accuracy"]
sen_ens <- cm_ens$byClass["Sensitivity"]

# store ensemble results
results <- bind_rows(results, tibble(Method = "Model (5) Ensemble", Accuracy = acc_ens , Sensitivity = sen_ens))
results %>% knitr::kable()
```

            Table 6 - Results after construction of the Ensemble model. 


# 5. Final Validation

In the model development, the random forest achieves the best accuracy and sensitivity. Therefore, it is selected to be the final model.

The entire `diabetes` data set is now used to construct a random forest. Like before, bootstrap is used to select an optimal mtry. All other parameters remain unchanged. The results from the bootstrap are shown in Figure 14. The optimal mtry value is 7.



```{r, warning = FALSE, echo = FALSE}
set.seed(1)
final_model_rf <- train(class~.,
                        method = "rf",
                        tuneGrid = data.frame(mtry = 3:11),
                        data = diabetes)

#visualize the performance of each mtry
ggplot(final_model_rf, highlight = TRUE)+
  scale_x_discrete(limits = 2:12) +
  ggtitle("Accuracy for each number of randomly selected predictors")
```
            Figure 11: Bootstrap results for various values of mtry.


The confusion matrix indicates that the random forest achieves perfect accuracy, correctly identifying all patients in the `validation` data set. Although the algorithm couldn't have performed any better, the results should be interpreted with caution. More on this is discussed in the Conclusion.

```{r, echo = FALSE}

#generating predictions
final_preds_rf <- predict(final_model_rf, validation)

cm_final <- confusionMatrix(final_preds_rf, validation$class)
# save accuracy and sensitivity
acc_final <- cm_final$overall["Accuracy"]
sen_final <- cm_final$byClass["Sensitivity"]

# store validation results
results <- bind_rows(results, tibble(Method = "Final Validation", Accuracy = acc_final , Sensitivity = sen_final))
results %>% knitr::kable()
```
                Table 7 - Results after Final validation using Diabetes. 


# 6. Conclusion

This report constructs a model using `diabetes` and  predicts the class of each patient in `validation`. Though the model performs well with accuracy of `r acc_final` and sensitivity of `r sen_final`, it is important to note conclusions regarding the size and source of the data utilized.

### Future work
The `diabetes` data set contains 442 observations. The final model is only tested on 78 observations. The final model would be much more reliable if it was trained and tested on a larger data set. A project like this would benefit from having access to a larger number of observations.

The data is sampled from one hospital, Sylhet Diabetic Hospital (SDH). That said, it is necessary to utilize this model to predict the class of patients in other hospitals from other countries. A significant improvement on this report would be if the data set was sampled from various hospitals across the world. Thus, the final model would be useful on a global scale.

Using a larger data set taken from a global sample would give the model much more credibility, however it is almost certain that the estimated accuracy of the model would change and perhaps ensemble could prove to be a better model.

### Additional considerations

The model is proven  to be a great tool to predict diabetes. The diagnosis for diabetes could be largely aided with a simple questionnaire. A model trained on a more appropriate data set with a solid excellent performance, it would be a valuable diagnostic tool to be shared and utilized by doctors around the world. Granted that patients answer the questions accurately, the test could even be made available online. The results could indicate a percentage of a person being at risk, and if the risk was reasonably high (above 10% or so) the person could be advised to seek proper medical advice.

Diabetes can have a severe impact on many bodily functions. A machine learning model such as the final model in this report could help to detect diabetes at an early stage. This could prevent strokes, blindness and even amputations in many patients.


# References

[1] UCI Machine Learning Repository *Early stage diabetes risk prediction dataset.* [https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.](https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.) (date last accessed - March 2020)

[2] Diabetes.co.uk *Diabetes Prevalence.* https://www.diabetes.co.uk/diabetes-prevalence.html (date last accessed - March 2020)

[3] Bazain/NHS *Men 'develop diabetes more easily'.* https://www.nhs.uk/news/diabetes/men-develop-diabetes-more-easily/ (date last accessed - March 2020) 
